{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPF9G2YJJrjpq4fn1XLkSjR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **A) Combined YOLO + DeepSORT**\n","\n"],"metadata":{"id":"_MNeFFNT-bWP"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkZ36ICG4svF","executionInfo":{"status":"ok","timestamp":1751390128411,"user_tz":420,"elapsed":276327,"user":{"displayName":"Greg Burns","userId":"16420449671066053506"}},"outputId":"74106d30-9733-49ae-e7fd-c294df657fa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","0: 384x640 (no detections), 178.3ms\n","Speed: 4.7ms preprocess, 178.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 187.8ms\n","Speed: 4.0ms preprocess, 187.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 motorcycle, 188.8ms\n","Speed: 5.1ms preprocess, 188.8ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 212.1ms\n","Speed: 5.0ms preprocess, 212.1ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 221.6ms\n","Speed: 4.8ms preprocess, 221.6ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 184.0ms\n","Speed: 4.0ms preprocess, 184.0ms inference, 12.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 181.7ms\n","Speed: 4.2ms preprocess, 181.7ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 294.4ms\n","Speed: 10.2ms preprocess, 294.4ms inference, 19.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 292.2ms\n","Speed: 4.3ms preprocess, 292.2ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 351.4ms\n","Speed: 8.5ms preprocess, 351.4ms inference, 20.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 281.8ms\n","Speed: 6.8ms preprocess, 281.8ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 traffic light, 344.3ms\n","Speed: 6.5ms preprocess, 344.3ms inference, 18.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 traffic light, 301.5ms\n","Speed: 4.7ms preprocess, 301.5ms inference, 24.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 186.3ms\n","Speed: 5.3ms preprocess, 186.3ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 174.8ms\n","Speed: 4.7ms preprocess, 174.8ms inference, 21.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 198.4ms\n","Speed: 4.9ms preprocess, 198.4ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 176.7ms\n","Speed: 3.6ms preprocess, 176.7ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 196.4ms\n","Speed: 9.7ms preprocess, 196.4ms inference, 14.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 traffic light, 1 bottle, 1 cell phone, 184.7ms\n","Speed: 3.6ms preprocess, 184.7ms inference, 21.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 traffic light, 1 bottle, 184.2ms\n","Speed: 5.1ms preprocess, 184.2ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 traffic light, 178.8ms\n","Speed: 4.1ms preprocess, 178.8ms inference, 12.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 3 traffic lights, 189.6ms\n","Speed: 4.3ms preprocess, 189.6ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 184.6ms\n","Speed: 4.8ms preprocess, 184.6ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 umbrella, 1 bottle, 197.8ms\n","Speed: 4.2ms preprocess, 197.8ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 umbrella, 1 handbag, 200.8ms\n","Speed: 4.4ms preprocess, 200.8ms inference, 17.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 handbag, 190.6ms\n","Speed: 5.1ms preprocess, 190.6ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 198.9ms\n","Speed: 3.9ms preprocess, 198.9ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 198.2ms\n","Speed: 3.6ms preprocess, 198.2ms inference, 13.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 225.4ms\n","Speed: 5.9ms preprocess, 225.4ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 190.4ms\n","Speed: 5.9ms preprocess, 190.4ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tennis racket, 192.4ms\n","Speed: 3.6ms preprocess, 192.4ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 tennis racket, 197.7ms\n","Speed: 4.2ms preprocess, 197.7ms inference, 12.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 191.6ms\n","Speed: 4.8ms preprocess, 191.6ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 210.2ms\n","Speed: 7.7ms preprocess, 210.2ms inference, 14.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 183.4ms\n","Speed: 4.4ms preprocess, 183.4ms inference, 17.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 213.0ms\n","Speed: 5.1ms preprocess, 213.0ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tennis racket, 294.2ms\n","Speed: 4.7ms preprocess, 294.2ms inference, 23.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 314.9ms\n","Speed: 18.9ms preprocess, 314.9ms inference, 22.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 292.3ms\n","Speed: 4.6ms preprocess, 292.3ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 handbag, 350.3ms\n","Speed: 4.0ms preprocess, 350.3ms inference, 21.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 268.4ms\n","Speed: 3.7ms preprocess, 268.4ms inference, 18.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 322.8ms\n","Speed: 6.2ms preprocess, 322.8ms inference, 13.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 191.5ms\n","Speed: 6.1ms preprocess, 191.5ms inference, 14.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 226.1ms\n","Speed: 4.7ms preprocess, 226.1ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 205.4ms\n","Speed: 4.3ms preprocess, 205.4ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 227.4ms\n","Speed: 4.1ms preprocess, 227.4ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 parking meter, 170.8ms\n","Speed: 4.2ms preprocess, 170.8ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 209.7ms\n","Speed: 4.1ms preprocess, 209.7ms inference, 15.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 178.1ms\n","Speed: 4.4ms preprocess, 178.1ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 192.5ms\n","Speed: 4.1ms preprocess, 192.5ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 umbrella, 1 frisbee, 189.1ms\n","Speed: 4.3ms preprocess, 189.1ms inference, 11.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 frisbee, 186.5ms\n","Speed: 4.1ms preprocess, 186.5ms inference, 13.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 frisbee, 175.7ms\n","Speed: 4.5ms preprocess, 175.7ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 182.7ms\n","Speed: 4.1ms preprocess, 182.7ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 umbrella, 186.0ms\n","Speed: 4.9ms preprocess, 186.0ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 umbrella, 190.9ms\n","Speed: 8.9ms preprocess, 190.9ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 umbrella, 191.2ms\n","Speed: 4.1ms preprocess, 191.2ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 umbrella, 198.0ms\n","Speed: 4.5ms preprocess, 198.0ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 207.3ms\n","Speed: 5.3ms preprocess, 207.3ms inference, 10.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 237.5ms\n","Speed: 7.5ms preprocess, 237.5ms inference, 9.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 195.1ms\n","Speed: 5.1ms preprocess, 195.1ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 188.9ms\n","Speed: 6.0ms preprocess, 188.9ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 umbrella, 1 bottle, 171.7ms\n","Speed: 3.9ms preprocess, 171.7ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 206.5ms\n","Speed: 4.8ms preprocess, 206.5ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 192.7ms\n","Speed: 4.4ms preprocess, 192.7ms inference, 12.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 316.3ms\n","Speed: 12.1ms preprocess, 316.3ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 303.4ms\n","Speed: 13.1ms preprocess, 303.4ms inference, 12.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 367.6ms\n","Speed: 6.1ms preprocess, 367.6ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 302.0ms\n","Speed: 10.9ms preprocess, 302.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 333.2ms\n","Speed: 7.6ms preprocess, 333.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 298.9ms\n","Speed: 10.0ms preprocess, 298.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 317.3ms\n","Speed: 5.0ms preprocess, 317.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 289.6ms\n","Speed: 6.1ms preprocess, 289.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 217.0ms\n","Speed: 3.8ms preprocess, 217.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 229.2ms\n","Speed: 4.4ms preprocess, 229.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 194.1ms\n","Speed: 4.3ms preprocess, 194.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 182.4ms\n","Speed: 4.6ms preprocess, 182.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 184.9ms\n","Speed: 4.6ms preprocess, 184.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 211.9ms\n","Speed: 4.4ms preprocess, 211.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 180.1ms\n","Speed: 4.2ms preprocess, 180.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 180.5ms\n","Speed: 6.2ms preprocess, 180.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 199.5ms\n","Speed: 4.2ms preprocess, 199.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 187.0ms\n","Speed: 4.0ms preprocess, 187.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 176.1ms\n","Speed: 4.2ms preprocess, 176.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 171.5ms\n","Speed: 3.8ms preprocess, 171.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 (no detections), 169.7ms\n","Speed: 4.5ms preprocess, 169.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 4 cups, 1 spoon, 1 cake, 2 dining tables, 198.1ms\n","Speed: 4.4ms preprocess, 198.1ms inference, 31.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bottle, 4 cups, 1 knife, 1 spoon, 1 dining table, 219.5ms\n","Speed: 5.5ms preprocess, 219.5ms inference, 35.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 knife, 1 spoon, 2 dining tables, 239.2ms\n","Speed: 6.0ms preprocess, 239.2ms inference, 55.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 5 cups, 1 knife, 1 spoon, 1 cake, 1 dining table, 198.9ms\n","Speed: 5.1ms preprocess, 198.9ms inference, 37.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 5 cups, 2 knifes, 2 spoons, 1 cake, 1 dining table, 206.1ms\n","Speed: 6.5ms preprocess, 206.1ms inference, 49.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 spoon, 1 cake, 1 dining table, 344.7ms\n","Speed: 4.3ms preprocess, 344.7ms inference, 58.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 1 spoon, 1 cake, 1 dining table, 306.8ms\n","Speed: 8.0ms preprocess, 306.8ms inference, 73.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 spoon, 1 dining table, 346.4ms\n","Speed: 9.3ms preprocess, 346.4ms inference, 59.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 5 cups, 1 spoon, 1 cake, 1 dining table, 218.9ms\n","Speed: 5.2ms preprocess, 218.9ms inference, 35.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 spoon, 1 cake, 2 dining tables, 207.5ms\n","Speed: 5.1ms preprocess, 207.5ms inference, 44.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 knife, 1 cake, 2 dining tables, 223.6ms\n","Speed: 5.0ms preprocess, 223.6ms inference, 42.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 dining table, 201.1ms\n","Speed: 5.0ms preprocess, 201.1ms inference, 36.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 10 cups, 1 dining table, 228.6ms\n","Speed: 4.9ms preprocess, 228.6ms inference, 39.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 10 cups, 1 knife, 1 dining table, 219.1ms\n","Speed: 4.4ms preprocess, 219.1ms inference, 43.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 10 cups, 1 chair, 1 dining table, 181.9ms\n","Speed: 4.3ms preprocess, 181.9ms inference, 41.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 dining table, 193.6ms\n","Speed: 8.6ms preprocess, 193.6ms inference, 37.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 cake, 1 dining table, 213.5ms\n","Speed: 5.1ms preprocess, 213.5ms inference, 42.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 10 cups, 1 dining table, 201.2ms\n","Speed: 6.9ms preprocess, 201.2ms inference, 41.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 6 cups, 1 dining table, 1 teddy bear, 189.8ms\n","Speed: 5.6ms preprocess, 189.8ms inference, 33.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 dining table, 326.6ms\n","Speed: 5.5ms preprocess, 326.6ms inference, 52.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 knife, 1 cake, 1 dining table, 317.8ms\n","Speed: 6.0ms preprocess, 317.8ms inference, 66.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 cake, 1 dining table, 390.2ms\n","Speed: 7.9ms preprocess, 390.2ms inference, 68.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 cake, 1 dining table, 332.8ms\n","Speed: 6.4ms preprocess, 332.8ms inference, 46.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 10 cups, 1 knife, 1 dining table, 216.2ms\n","Speed: 5.2ms preprocess, 216.2ms inference, 45.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 10 cups, 1 knife, 1 dining table, 169.4ms\n","Speed: 3.3ms preprocess, 169.4ms inference, 43.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 249.9ms\n","Speed: 5.7ms preprocess, 249.9ms inference, 46.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 234.6ms\n","Speed: 5.5ms preprocess, 234.6ms inference, 54.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 knife, 1 dining table, 213.2ms\n","Speed: 5.5ms preprocess, 213.2ms inference, 38.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 knife, 1 dining table, 206.6ms\n","Speed: 5.2ms preprocess, 206.6ms inference, 37.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 1 dining table, 217.3ms\n","Speed: 4.8ms preprocess, 217.3ms inference, 42.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 knife, 1 cake, 1 dining table, 191.8ms\n","Speed: 5.0ms preprocess, 191.8ms inference, 39.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 9 cups, 1 knife, 1 dining table, 194.2ms\n","Speed: 4.8ms preprocess, 194.2ms inference, 36.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 220.3ms\n","Speed: 5.4ms preprocess, 220.3ms inference, 44.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 221.4ms\n","Speed: 5.2ms preprocess, 221.4ms inference, 36.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 dining table, 297.2ms\n","Speed: 4.9ms preprocess, 297.2ms inference, 89.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 365.9ms\n","Speed: 15.3ms preprocess, 365.9ms inference, 79.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 271.6ms\n","Speed: 12.5ms preprocess, 271.6ms inference, 43.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 1 spoon, 1 dining table, 196.4ms\n","Speed: 4.5ms preprocess, 196.4ms inference, 38.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 1 spoon, 1 dining table, 188.9ms\n","Speed: 4.4ms preprocess, 188.9ms inference, 39.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bottle, 9 cups, 1 knife, 1 spoon, 1 dining table, 217.8ms\n","Speed: 5.4ms preprocess, 217.8ms inference, 45.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bottle, 6 cups, 1 knife, 1 dining table, 470.9ms\n","Speed: 16.2ms preprocess, 470.9ms inference, 43.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bottle, 8 cups, 1 knife, 1 spoon, 1 dining table, 795.8ms\n","Speed: 8.2ms preprocess, 795.8ms inference, 122.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 1 spoon, 1 dining table, 308.6ms\n","Speed: 10.4ms preprocess, 308.6ms inference, 41.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bottle, 6 cups, 1 dining table, 493.9ms\n","Speed: 8.2ms preprocess, 493.9ms inference, 49.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 bottles, 7 cups, 1 spoon, 1 dining table, 384.8ms\n","Speed: 11.6ms preprocess, 384.8ms inference, 80.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 spoon, 1 chair, 1 dining table, 453.1ms\n","Speed: 11.8ms preprocess, 453.1ms inference, 152.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 spoon, 1 dining table, 325.6ms\n","Speed: 10.1ms preprocess, 325.6ms inference, 60.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 dining table, 244.1ms\n","Speed: 4.4ms preprocess, 244.1ms inference, 34.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 7 cups, 1 spoon, 180.6ms\n","Speed: 5.3ms preprocess, 180.6ms inference, 34.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 308.9ms\n","Speed: 5.1ms preprocess, 308.9ms inference, 114.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 6 cups, 187.6ms\n","Speed: 4.2ms preprocess, 187.6ms inference, 29.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 8 cups, 1 spoon, 353.2ms\n","Speed: 8.6ms preprocess, 353.2ms inference, 78.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 7 cups, 1 knife, 1 dining table, 217.4ms\n","Speed: 11.0ms preprocess, 217.4ms inference, 39.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 3 bottles, 7 cups, 1 knife, 1 spoon, 1 bowl, 1 dining table, 199.4ms\n","Speed: 4.4ms preprocess, 199.4ms inference, 55.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 9 cups, 206.5ms\n","Speed: 5.9ms preprocess, 206.5ms inference, 40.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 6 cups, 1 spoon, 338.0ms\n","Speed: 10.7ms preprocess, 338.0ms inference, 42.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 bowl, 250.3ms\n","Speed: 5.0ms preprocess, 250.3ms inference, 34.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 knife, 1 dining table, 467.3ms\n","Speed: 9.5ms preprocess, 467.3ms inference, 61.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 cake, 1 dining table, 335.3ms\n","Speed: 13.6ms preprocess, 335.3ms inference, 61.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 spoon, 1 cake, 1 chair, 1 dining table, 199.0ms\n","Speed: 5.4ms preprocess, 199.0ms inference, 43.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 spoon, 1 dining table, 209.1ms\n","Speed: 5.4ms preprocess, 209.1ms inference, 42.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 2 knifes, 1 spoon, 1 dining table, 210.8ms\n","Speed: 4.7ms preprocess, 210.8ms inference, 42.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 1 dining table, 201.7ms\n","Speed: 4.5ms preprocess, 201.7ms inference, 48.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 2 knifes, 1 dining table, 230.7ms\n","Speed: 5.4ms preprocess, 230.7ms inference, 42.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 knife, 1 spoon, 1 dining table, 184.2ms\n","Speed: 4.1ms preprocess, 184.2ms inference, 38.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 219.1ms\n","Speed: 5.3ms preprocess, 219.1ms inference, 41.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 184.6ms\n","Speed: 4.0ms preprocess, 184.6ms inference, 42.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 222.5ms\n","Speed: 5.1ms preprocess, 222.5ms inference, 41.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 6 cups, 1 knife, 1 spoon, 1 dining table, 191.8ms\n","Speed: 15.0ms preprocess, 191.8ms inference, 34.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 knife, 1 spoon, 1 dining table, 209.2ms\n","Speed: 6.6ms preprocess, 209.2ms inference, 36.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 5 cups, 1 knife, 1 spoon, 1 dining table, 184.0ms\n","Speed: 4.3ms preprocess, 184.0ms inference, 46.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 7 cups, 1 knife, 1 spoon, 1 cake, 1 dining table, 332.5ms\n","Speed: 11.0ms preprocess, 332.5ms inference, 66.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 4 cups, 1 dining table, 302.5ms\n","Speed: 10.0ms preprocess, 302.5ms inference, 44.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 spoon, 1 dining table, 287.0ms\n","Speed: 4.9ms preprocess, 287.0ms inference, 67.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 8 cups, 1 spoon, 1 dining table, 234.2ms\n","Speed: 6.5ms preprocess, 234.2ms inference, 38.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 7 cups, 1 spoon, 1 dining table, 208.9ms\n","Speed: 5.0ms preprocess, 208.9ms inference, 33.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 9 cups, 1 knife, 1 dining table, 168.2ms\n","Speed: 4.3ms preprocess, 168.2ms inference, 38.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 8 cups, 1 knife, 1 dining table, 213.9ms\n","Speed: 5.0ms preprocess, 213.9ms inference, 39.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 6 cups, 1 dining table, 224.4ms\n","Speed: 5.4ms preprocess, 224.4ms inference, 31.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 5 cups, 1 dining table, 217.4ms\n","Speed: 5.1ms preprocess, 217.4ms inference, 36.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 6 cups, 1 cake, 1 dining table, 198.8ms\n","Speed: 5.3ms preprocess, 198.8ms inference, 43.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 7 cups, 1 knife, 1 dining table, 215.3ms\n","Speed: 5.5ms preprocess, 215.3ms inference, 38.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 6 cups, 1 chair, 1 dining table, 207.9ms\n","Speed: 3.4ms preprocess, 207.9ms inference, 34.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 5 cups, 1 chair, 1 dining table, 200.8ms\n","Speed: 8.2ms preprocess, 200.8ms inference, 32.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 5 cups, 1 chair, 1 dining table, 167.6ms\n","Speed: 4.8ms preprocess, 167.6ms inference, 31.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 5 cups, 1 chair, 2 dining tables, 195.5ms\n","Speed: 4.4ms preprocess, 195.5ms inference, 36.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 5 cups, 1 chair, 1 dining table, 203.6ms\n","Speed: 5.0ms preprocess, 203.6ms inference, 36.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 5 cups, 1 chair, 1 dining table, 263.2ms\n","Speed: 4.9ms preprocess, 263.2ms inference, 46.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 5 cups, 1 chair, 1 dining table, 329.3ms\n","Speed: 12.9ms preprocess, 329.3ms inference, 52.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 6 cups, 1 dining table, 388.4ms\n","Speed: 6.4ms preprocess, 388.4ms inference, 56.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 bottle, 5 cups, 1 dining table, 180.9ms\n","Speed: 4.3ms preprocess, 180.9ms inference, 29.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 5 cups, 1 dining table, 203.7ms\n","Speed: 5.1ms preprocess, 203.7ms inference, 35.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 4 cups, 1 dining table, 222.2ms\n","Speed: 9.1ms preprocess, 222.2ms inference, 41.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 bottles, 5 cups, 1 dining table, 222.8ms\n","Speed: 5.4ms preprocess, 222.8ms inference, 35.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 232.2ms\n","Speed: 5.7ms preprocess, 232.2ms inference, 42.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 180.9ms\n","Speed: 4.4ms preprocess, 180.9ms inference, 21.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 232.6ms\n","Speed: 9.3ms preprocess, 232.6ms inference, 22.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 199.7ms\n","Speed: 5.4ms preprocess, 199.7ms inference, 22.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 234.6ms\n","Speed: 6.1ms preprocess, 234.6ms inference, 23.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 202.5ms\n","Speed: 8.3ms preprocess, 202.5ms inference, 20.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 316.5ms\n","Speed: 5.7ms preprocess, 316.5ms inference, 36.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 325.2ms\n","Speed: 5.1ms preprocess, 325.2ms inference, 40.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 398.8ms\n","Speed: 9.9ms preprocess, 398.8ms inference, 37.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 209.8ms\n","Speed: 6.8ms preprocess, 209.8ms inference, 47.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 201.2ms\n","Speed: 4.8ms preprocess, 201.2ms inference, 22.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 206.4ms\n","Speed: 25.7ms preprocess, 206.4ms inference, 23.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 205.3ms\n","Speed: 4.5ms preprocess, 205.3ms inference, 20.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 229.5ms\n","Speed: 4.6ms preprocess, 229.5ms inference, 20.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 179.2ms\n","Speed: 4.5ms preprocess, 179.2ms inference, 22.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 231.0ms\n","Speed: 4.8ms preprocess, 231.0ms inference, 24.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 196.5ms\n","Speed: 5.1ms preprocess, 196.5ms inference, 28.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 175.8ms\n","Speed: 4.5ms preprocess, 175.8ms inference, 23.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 202.4ms\n","Speed: 4.4ms preprocess, 202.4ms inference, 24.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 354.9ms\n","Speed: 4.7ms preprocess, 354.9ms inference, 86.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 427.8ms\n","Speed: 13.9ms preprocess, 427.8ms inference, 37.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 294.1ms\n","Speed: 14.1ms preprocess, 294.1ms inference, 37.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 221.2ms\n","Speed: 5.1ms preprocess, 221.2ms inference, 40.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 380.8ms\n","Speed: 12.7ms preprocess, 380.8ms inference, 36.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 187.1ms\n","Speed: 6.4ms preprocess, 187.1ms inference, 25.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 202.9ms\n","Speed: 5.3ms preprocess, 202.9ms inference, 24.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 377.4ms\n","Speed: 4.1ms preprocess, 377.4ms inference, 20.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 189.4ms\n","Speed: 8.2ms preprocess, 189.4ms inference, 22.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 208.0ms\n","Speed: 4.6ms preprocess, 208.0ms inference, 22.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 240.2ms\n","Speed: 4.2ms preprocess, 240.2ms inference, 24.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 chair, 1 dining table, 189.1ms\n","Speed: 11.0ms preprocess, 189.1ms inference, 24.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 177.1ms\n","Speed: 4.4ms preprocess, 177.1ms inference, 21.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 791.7ms\n","Speed: 11.3ms preprocess, 791.7ms inference, 135.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 289.6ms\n","Speed: 6.0ms preprocess, 289.6ms inference, 69.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 817.0ms\n","Speed: 4.2ms preprocess, 817.0ms inference, 83.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 172.4ms\n","Speed: 4.1ms preprocess, 172.4ms inference, 23.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 216.1ms\n","Speed: 5.2ms preprocess, 216.1ms inference, 22.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 213.9ms\n","Speed: 5.5ms preprocess, 213.9ms inference, 22.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 214.1ms\n","Speed: 5.5ms preprocess, 214.1ms inference, 21.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 170.6ms\n","Speed: 3.7ms preprocess, 170.6ms inference, 20.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 197.8ms\n","Speed: 8.8ms preprocess, 197.8ms inference, 22.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 243.1ms\n","Speed: 5.3ms preprocess, 243.1ms inference, 22.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 174.4ms\n","Speed: 3.5ms preprocess, 174.4ms inference, 20.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 242.3ms\n","Speed: 7.9ms preprocess, 242.3ms inference, 22.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 193.7ms\n","Speed: 3.3ms preprocess, 193.7ms inference, 32.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 204.4ms\n","Speed: 4.7ms preprocess, 204.4ms inference, 20.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 316.8ms\n","Speed: 12.2ms preprocess, 316.8ms inference, 32.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 306.4ms\n","Speed: 19.8ms preprocess, 306.4ms inference, 36.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 296.3ms\n","Speed: 4.9ms preprocess, 296.3ms inference, 34.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 189.8ms\n","Speed: 5.1ms preprocess, 189.8ms inference, 24.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 216.6ms\n","Speed: 5.1ms preprocess, 216.6ms inference, 23.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 169.1ms\n","Speed: 4.5ms preprocess, 169.1ms inference, 20.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 213.1ms\n","Speed: 5.1ms preprocess, 213.1ms inference, 24.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 182.1ms\n","Speed: 4.6ms preprocess, 182.1ms inference, 20.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 330.7ms\n","Speed: 4.1ms preprocess, 330.7ms inference, 38.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 193.0ms\n","Speed: 4.6ms preprocess, 193.0ms inference, 21.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 212.5ms\n","Speed: 4.8ms preprocess, 212.5ms inference, 32.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 358.0ms\n","Speed: 7.3ms preprocess, 358.0ms inference, 33.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 206.1ms\n","Speed: 4.9ms preprocess, 206.1ms inference, 29.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 cups, 1 dining table, 200.2ms\n","Speed: 4.7ms preprocess, 200.2ms inference, 37.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 414.0ms\n","Speed: 5.1ms preprocess, 414.0ms inference, 36.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 351.9ms\n","Speed: 5.3ms preprocess, 351.9ms inference, 33.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 207.8ms\n","Speed: 11.3ms preprocess, 207.8ms inference, 22.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 207.6ms\n","Speed: 5.9ms preprocess, 207.6ms inference, 22.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 207.3ms\n","Speed: 6.2ms preprocess, 207.3ms inference, 27.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 372.6ms\n","Speed: 4.5ms preprocess, 372.6ms inference, 21.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 204.1ms\n","Speed: 5.1ms preprocess, 204.1ms inference, 33.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 188.2ms\n","Speed: 5.8ms preprocess, 188.2ms inference, 23.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 209.9ms\n","Speed: 8.6ms preprocess, 209.9ms inference, 22.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 222.3ms\n","Speed: 6.5ms preprocess, 222.3ms inference, 23.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 3 cups, 1 dining table, 212.6ms\n","Speed: 5.2ms preprocess, 212.6ms inference, 24.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 361.4ms\n","Speed: 5.5ms preprocess, 361.4ms inference, 34.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 315.1ms\n","Speed: 10.8ms preprocess, 315.1ms inference, 38.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 359.0ms\n","Speed: 5.0ms preprocess, 359.0ms inference, 34.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 291.0ms\n","Speed: 4.2ms preprocess, 291.0ms inference, 33.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 186.0ms\n","Speed: 5.6ms preprocess, 186.0ms inference, 20.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 191.8ms\n","Speed: 6.1ms preprocess, 191.8ms inference, 22.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 192.5ms\n","Speed: 10.0ms preprocess, 192.5ms inference, 22.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 210.8ms\n","Speed: 8.7ms preprocess, 210.8ms inference, 21.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 178.4ms\n","Speed: 5.1ms preprocess, 178.4ms inference, 21.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 198.8ms\n","Speed: 6.6ms preprocess, 198.8ms inference, 29.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 192.2ms\n","Speed: 4.8ms preprocess, 192.2ms inference, 22.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 ties, 2 cups, 1 dining table, 186.4ms\n","Speed: 5.9ms preprocess, 186.4ms inference, 22.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 frisbee, 2 sports balls, 6 wine glasss, 204.5ms\n","Speed: 5.6ms preprocess, 204.5ms inference, 33.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 3 wine glasss, 218.6ms\n","Speed: 5.8ms preprocess, 218.6ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 4 wine glasss, 304.6ms\n","Speed: 4.2ms preprocess, 304.6ms inference, 40.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 5 wine glasss, 295.3ms\n","Speed: 4.7ms preprocess, 295.3ms inference, 49.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 3 wine glasss, 1 cup, 361.3ms\n","Speed: 10.9ms preprocess, 361.3ms inference, 50.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 2 wine glasss, 312.5ms\n","Speed: 5.1ms preprocess, 312.5ms inference, 42.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 3 ties, 3 sports balls, 2 wine glasss, 298.8ms\n","Speed: 7.6ms preprocess, 298.8ms inference, 50.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 4 sports balls, 1 wine glass, 205.5ms\n","Speed: 5.8ms preprocess, 205.5ms inference, 28.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 4 wine glasss, 192.9ms\n","Speed: 6.2ms preprocess, 192.9ms inference, 31.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 3 ties, 3 sports balls, 4 wine glasss, 206.6ms\n","Speed: 4.6ms preprocess, 206.6ms inference, 38.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 2 wine glasss, 206.0ms\n","Speed: 5.6ms preprocess, 206.0ms inference, 22.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 sports balls, 1 wine glass, 183.4ms\n","Speed: 4.9ms preprocess, 183.4ms inference, 24.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 175.5ms\n","Speed: 4.1ms preprocess, 175.5ms inference, 27.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 sports balls, 1 wine glass, 172.2ms\n","Speed: 4.5ms preprocess, 172.2ms inference, 22.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 sports balls, 3 wine glasss, 191.5ms\n","Speed: 3.6ms preprocess, 191.5ms inference, 27.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 sports balls, 2 wine glasss, 203.0ms\n","Speed: 4.6ms preprocess, 203.0ms inference, 26.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 182.3ms\n","Speed: 6.3ms preprocess, 182.3ms inference, 22.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 sports balls, 1 wine glass, 206.9ms\n","Speed: 6.1ms preprocess, 206.9ms inference, 24.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 1 wine glass, 203.7ms\n","Speed: 4.8ms preprocess, 203.7ms inference, 21.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 4 wine glasss, 191.7ms\n","Speed: 4.7ms preprocess, 191.7ms inference, 29.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 6 wine glasss, 203.9ms\n","Speed: 6.9ms preprocess, 203.9ms inference, 33.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 6 wine glasss, 197.4ms\n","Speed: 4.6ms preprocess, 197.4ms inference, 48.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 4 wine glasss, 296.1ms\n","Speed: 5.5ms preprocess, 296.1ms inference, 45.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 5 wine glasss, 275.6ms\n","Speed: 25.6ms preprocess, 275.6ms inference, 47.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 sports balls, 4 wine glasss, 1 cup, 334.4ms\n","Speed: 8.2ms preprocess, 334.4ms inference, 58.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 3 wine glasss, 322.5ms\n","Speed: 15.3ms preprocess, 322.5ms inference, 71.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 2 wine glasss, 207.3ms\n","Speed: 4.7ms preprocess, 207.3ms inference, 26.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 3 wine glasss, 196.9ms\n","Speed: 5.5ms preprocess, 196.9ms inference, 23.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 4 wine glasss, 1 cup, 176.7ms\n","Speed: 5.2ms preprocess, 176.7ms inference, 27.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 2 wine glasss, 199.7ms\n","Speed: 4.1ms preprocess, 199.7ms inference, 24.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 4 wine glasss, 310.8ms\n","Speed: 6.2ms preprocess, 310.8ms inference, 28.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 4 sports balls, 4 wine glasss, 1 cup, 183.2ms\n","Speed: 7.0ms preprocess, 183.2ms inference, 34.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 sports balls, 1 wine glass, 2 cups, 196.8ms\n","Speed: 3.9ms preprocess, 196.8ms inference, 59.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 2 sports balls, 3 wine glasss, 1 cup, 232.9ms\n","Speed: 5.0ms preprocess, 232.9ms inference, 30.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 3 wine glasss, 1 cup, 175.6ms\n","Speed: 4.4ms preprocess, 175.6ms inference, 27.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 3 wine glasss, 1 cup, 218.7ms\n","Speed: 8.4ms preprocess, 218.7ms inference, 27.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 2 cups, 1 cell phone, 184.2ms\n","Speed: 6.2ms preprocess, 184.2ms inference, 36.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 5 wine glasss, 1 cup, 276.3ms\n","Speed: 4.9ms preprocess, 276.3ms inference, 55.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 3 wine glasss, 1 cup, 1 cell phone, 209.5ms\n","Speed: 10.0ms preprocess, 209.5ms inference, 30.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 4 wine glasss, 1 cup, 172.8ms\n","Speed: 4.7ms preprocess, 172.8ms inference, 27.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 4 wine glasss, 1 cup, 1 cell phone, 494.8ms\n","Speed: 4.4ms preprocess, 494.8ms inference, 143.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 3 wine glasss, 1 cup, 349.3ms\n","Speed: 4.8ms preprocess, 349.3ms inference, 42.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 3 wine glasss, 1 cup, 328.7ms\n","Speed: 4.4ms preprocess, 328.7ms inference, 41.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 3 wine glasss, 1 cup, 259.9ms\n","Speed: 4.3ms preprocess, 259.9ms inference, 38.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 3 wine glasss, 288.5ms\n","Speed: 4.0ms preprocess, 288.5ms inference, 40.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 tie, 1 sports ball, 4 wine glasss, 1 cup, 207.6ms\n","Speed: 4.8ms preprocess, 207.6ms inference, 30.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 1 cup, 226.2ms\n","Speed: 6.8ms preprocess, 226.2ms inference, 34.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 1 cup, 175.5ms\n","Speed: 5.2ms preprocess, 175.5ms inference, 29.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 1 cup, 174.4ms\n","Speed: 3.8ms preprocess, 174.4ms inference, 31.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 1 cup, 200.7ms\n","Speed: 4.1ms preprocess, 200.7ms inference, 29.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 3 wine glasss, 1 cup, 177.8ms\n","Speed: 5.5ms preprocess, 177.8ms inference, 34.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 4 wine glasss, 1 cup, 207.3ms\n","Speed: 6.5ms preprocess, 207.3ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 4 wine glasss, 1 cup, 186.6ms\n","Speed: 6.8ms preprocess, 186.6ms inference, 35.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 4 wine glasss, 1 cup, 172.7ms\n","Speed: 4.1ms preprocess, 172.7ms inference, 29.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 4 wine glasss, 1 cup, 174.4ms\n","Speed: 8.7ms preprocess, 174.4ms inference, 29.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 1 sports ball, 4 wine glasss, 1 cup, 191.1ms\n","Speed: 4.7ms preprocess, 191.1ms inference, 34.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 4 wine glasss, 1 cup, 185.6ms\n","Speed: 4.3ms preprocess, 185.6ms inference, 31.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 3 sports balls, 4 wine glasss, 1 cup, 189.9ms\n","Speed: 4.2ms preprocess, 189.9ms inference, 34.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 4 wine glasss, 1 cup, 174.2ms\n","Speed: 3.6ms preprocess, 174.2ms inference, 32.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 1 cup, 205.6ms\n","Speed: 15.1ms preprocess, 205.6ms inference, 29.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 ties, 2 sports balls, 3 wine glasss, 1 cup, 201.1ms\n","Speed: 6.5ms preprocess, 201.1ms inference, 32.9ms postprocess per image at shape (1, 3, 384, 640)\n","Saved annotations to /content/drive/MyDrive/FreeFuse_Project/Videos/Input/draft_annotations.csv\n"]}],"source":["# A) Combine YOLO + DeepSORT\n","# === Install required packages (run once) ===\n","# !pip install ultralytics deep-sort-realtime opencv-python pandas\n","\n","from google.colab import drive\n","from ultralytics import YOLO\n","from deep_sort_realtime.deepsort_tracker import DeepSort\n","import cv2\n","import os\n","import json\n","import pandas as pd\n","from pathlib import Path\n","import numpy as np\n","\n","# === Parameters ===\n","VIDEO_FOLDER         = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Input\")\n","OUTPUT_FOLDER        = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Output\")\n","CONFIDENCE_THRESHOLD = 0.4\n","DETECTION_INTERVAL   = 1      # analyze every Nth frame\n","MAX_TRACK_AGE        = 30     # frames to keep a lost track\n","MIN_HITS             = 3      # detections before confirming a track\n","\n","# drawing settings\n","MASK_COLOR           = (0, 255, 0)    # BGR mask outline color\n","MASK_THICKNESS       = 2              # mask polygon line thickness\n","TEXT_COLOR           = (255, 255, 255)# BGR text color\n","TEXT_FONT            = cv2.FONT_HERSHEY_SIMPLEX\n","TEXT_SCALE           = 0.6\n","TEXT_THICKNESS       = 2\n","\n","# === 1) Mount Google Drive ===\n","drive.mount('/content/drive')\n","\n","# === 2) Load YOLOv8-nano segmentation & DeepSORT ===\n","model   = YOLO('yolov8n-seg')           # auto-downloads nano-segmentation weights\n","tracker = DeepSort(max_age=MAX_TRACK_AGE, n_init=MIN_HITS)\n","\n","# utility to compute IoU between two boxes\n","\n","def compute_iou(boxA, boxB):\n","    xA1,yA1,xA2,yA2 = boxA\n","    xB1,yB1,xB2,yB2 = boxB\n","    xi1, yi1 = max(xA1,xB1), max(yA1,yB1)\n","    xi2, yi2 = min(xA2,xB2), min(yA2,yB2)\n","    inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n","    union = (xA2-xA1)*(yA2-yA1) + (xB2-xB1)*(yB2-yB1) - inter\n","    return inter/union if union>0 else 0\n","\n","annotations = []\n","\n","# ensure output CSV and video folder exist\n","OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n","\n","for video_file in os.listdir(VIDEO_FOLDER):\n","    if not video_file.lower().endswith(('.mp4','.mov','.avi')):\n","        continue\n","\n","    cap        = cv2.VideoCapture(str(VIDEO_FOLDER/video_file))\n","    fps        = cap.get(cv2.CAP_PROP_FPS)\n","    width      = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    frame_num  = 0\n","    video_name = Path(video_file).stem\n","\n","    # prepare video writer\n","    output_path = OUTPUT_FOLDER / video_file\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_num % DETECTION_INTERVAL == 0:\n","            timestamp_sec = int(frame_num / fps)\n","            frame_id      = f\"{video_name}_{timestamp_sec:04d}\"\n","\n","            # YOLOv8 segmentation inference\n","            results = model(frame)[0]\n","\n","            dets_for_tracker = []\n","            det_meta = []\n","            for idx, (box, score, cls) in enumerate(zip(\n","                    results.boxes.xyxy, results.boxes.conf, results.boxes.cls)):\n","                conf = float(score)\n","                if conf < CONFIDENCE_THRESHOLD:\n","                    continue\n","\n","                x1,y1,x2,y2 = box.cpu().numpy().astype(int)\n","                cls_id      = int(cls.cpu().numpy())\n","                name        = model.names[cls_id]\n","\n","                # extract polygon in original image scale\n","                # YOLOv8 provides masks.xy which are already scaled\n","                poly = np.array(results.masks.xy[idx], dtype=np.int32)\n","                # ensure shape (-1,2)\n","                poly = poly.reshape(-1,2)\n","\n","                dets_for_tracker.append([[x1,y1,x2-x1,y2-y1], conf, name])\n","                det_meta.append({\n","                    \"bbox\": (x1,y1,x2,y2),\n","                    \"MID\": f\"/m/{cls_id:07d}\",\n","                    \"object_name\": name,\n","                    \"object_category\": \"unknown\",\n","                    \"mask_poly\": poly.tolist(),\n","                    \"confidence\": conf,\n","                })\n","\n","            # update tracker\n","            tracks = tracker.update_tracks(dets_for_tracker, frame=frame)\n","\n","            if det_meta:\n","                for trk in tracks:\n","                    if not trk.is_confirmed():\n","                        continue\n","                    tx1,ty1,tx2,ty2 = trk.to_tlbr()\n","                    track_id = trk.track_id\n","\n","                    # match detection by IoU\n","                    best_iou, best = max(\n","                        ((compute_iou((tx1,ty1,tx2,ty2), m[\"bbox\"]), m) for m in det_meta),\n","                        key=lambda x: x[0]\n","                    )\n","                    if best_iou > 0.3:\n","                        # draw mask outline using original-scale polygon\n","                        pts = np.array(best[\"mask_poly\"], np.int32)\n","                        if pts.size:\n","                            cv2.polylines(frame, [pts], isClosed=True, color=MASK_COLOR, thickness=MASK_THICKNESS)\n","                            # place label at first vertex\n","                            label_pos = tuple(pts[0])\n","                            cv2.putText(frame, best[\"object_name\"], label_pos, TEXT_FONT,\n","                                        TEXT_SCALE, TEXT_COLOR, TEXT_THICKNESS, cv2.LINE_AA)\n","\n","                        # record annotation\n","                        annotations.append({\n","                            \"video_filename\":    video_file,\n","                            \"frame_id\":          frame_id,\n","                            \"track_id\":          f\"{video_name}_{track_id}\",\n","                            \"object_id\":         f\"{frame_id}_obj{track_id}\",\n","                            \"timestamp_sec\":     timestamp_sec,\n","                            \"image_width_px\":    width,\n","                            \"image_height_px\":   height,\n","                            \"MID\":               best[\"MID\"],\n","                            \"object_name\":       best[\"object_name\"],\n","                            \"object_category\":   best[\"object_category\"],\n","                            \"x_min\":             int(tx1),\n","                            \"y_min\":             int(ty1),\n","                            \"x_max\":             int(tx2),\n","                            \"y_max\":             int(ty2),\n","                            \"segmentation_mask\": json.dumps([best[\"mask_poly\"]]),\n","                            \"confidence\":        best[\"confidence\"],\n","                            \"interaction_score\": 0.0\n","                        })\n","\n","        # write frame (with masks) to output\n","        writer.write(frame)\n","        frame_num += 1\n","\n","    cap.release()\n","    writer.release()\n","\n","# write CSV of annotations\n","out_csv = OUTPUT_FOLDER / \"draft_annotations.csv\"\n","pd.DataFrame(annotations).to_csv(out_csv, index=False)\n","print(f\"Saved annotated video(s) to {OUTPUT_FOLDER}\")\n","print(f\"Saved annotations to {out_csv}\")"]},{"cell_type":"markdown","source":["## **B) Detectron2 (Facebook AI Research)**"],"metadata":{"id":"YHP6AejR-t0D"}},{"cell_type":"code","source":["# Combine Detectron2 + DeepSORT + Mask Drawing\n","# Refactored from YOLO to Detectron2 instance segmentation\n","# === Install required packages (run once) ===\n","!pip install detectron2 deep-sort-realtime opencv-python pandas\n","\n","from google.colab import drive\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2 import model_zoo\n","from detectron2.data import MetadataCatalog\n","from deep_sort_realtime.deepsort_tracker import DeepSort\n","import cv2\n","import os\n","import json\n","import pandas as pd\n","from pathlib import Path\n","import numpy as np\n","\n","# === Parameters ===\n","VIDEO_FOLDER         = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Input\")\n","OUTPUT_FOLDER        = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Output\")\n","CONFIDENCE_THRESHOLD = 0.5\n","DETECTION_INTERVAL   = 5      # analyze every Nth frame\n","MAX_TRACK_AGE        = 30     # frames to keep a lost track\n","MIN_HITS             = 3      # detections before confirming a track\n","\n","# drawing settings\n","MASK_COLOR           = (0, 255, 0)    # BGR mask outline color\n","MASK_THICKNESS       = 2              # mask polygon line thickness\n","TEXT_COLOR           = (255, 255, 255)# BGR text color\n","TEXT_FONT            = cv2.FONT_HERSHEY_SIMPLEX\n","TEXT_SCALE           = 0.6\n","TEXT_THICKNESS       = 2\n","\n","# === 1) Mount Google Drive ===\n","drive.mount('/content/drive')\n","\n","# === 2) Configure Detectron2 ===\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = CONFIDENCE_THRESHOLD\n","predictor = DefaultPredictor(cfg)\n","# metadata for class names\n","dataset = cfg.DATASETS.TRAIN[0] if len(cfg.DATASETS.TRAIN)>0 else \"coco_2017_train\"\n","class_names = MetadataCatalog.get(dataset).thing_classes\n","\n","# Initialize DeepSORT tracker\n","tracker = DeepSort(max_age=MAX_TRACK_AGE, n_init=MIN_HITS)\n","\n","# IoU utility\n","def compute_iou(boxA, boxB):\n","    xA1,yA1,xA2,yA2 = boxA\n","    xB1,yB1,xB2,yB2 = boxB\n","    xi1, yi1 = max(xA1,xB1), max(yA1,yB1)\n","    xi2, yi2 = min(xA2,xB2), min(yA2,yB2)\n","    inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n","    union = (xA2-xA1)*(yA2-yA1) + (xB2-xB1)*(yB2-yB1) - inter\n","    return inter/union if union>0 else 0\n","\n","annotations = []\n","\n","# ensure output folders exist\n","OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n","\n","for video_file in os.listdir(VIDEO_FOLDER):\n","    if not video_file.lower().endswith(('.mp4','.mov','.avi')):\n","        continue\n","\n","    cap        = cv2.VideoCapture(str(VIDEO_FOLDER/video_file))\n","    fps        = cap.get(cv2.CAP_PROP_FPS)\n","    width      = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    frame_num  = 0\n","    video_name = Path(video_file).stem\n","\n","    # prepare video writer\n","    output_path = OUTPUT_FOLDER / video_file\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_num % DETECTION_INTERVAL == 0:\n","            timestamp_sec = int(frame_num / fps)\n","            frame_id      = f\"{video_name}_{timestamp_sec:04d}\"\n","\n","            # Detectron2 inference\n","            outputs = predictor(frame)\n","            instances = outputs[\"instances\"].to(\"cpu\")\n","            boxes = instances.pred_boxes.tensor.numpy().astype(int)\n","            scores = instances.scores.numpy()\n","            classes = instances.pred_classes.numpy().astype(int)\n","            masks = instances.pred_masks.numpy()  # (N, H, W)\n","\n","            dets_for_tracker = []\n","            det_meta = []\n","            for idx in range(len(boxes)):\n","                conf = float(scores[idx])\n","                if conf < CONFIDENCE_THRESHOLD:\n","                    continue\n","\n","                x1,y1,x2,y2 = boxes[idx]\n","                cls_id = classes[idx]\n","                name = class_names[cls_id]\n","\n","                # extract polygon from mask\n","                mask_arr = (masks[idx].astype(np.uint8)*255)\n","                contours, _ = cv2.findContours(mask_arr, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","                poly = contours[0].reshape(-1,2).tolist() if contours else []\n","\n","                dets_for_tracker.append([[x1,y1,x2-x1,y2-y1], conf, name])\n","                det_meta.append({\n","                    \"bbox\": (x1,y1,x2,y2),\n","                    \"MID\": f\"/m/{cls_id:07d}\",\n","                    \"object_name\": name,\n","                    \"object_category\": \"unknown\",\n","                    \"mask_poly\": poly,\n","                    \"confidence\": conf,\n","                })\n","\n","            # update tracker\n","            tracks = tracker.update_tracks(dets_for_tracker, frame=frame)\n","            if det_meta:\n","                for trk in tracks:\n","                    if not trk.is_confirmed():\n","                        continue\n","                    tx1,ty1,tx2,ty2 = trk.to_tlbr()\n","                    track_id = trk.track_id\n","\n","                    # match detection by IoU\n","                    best_iou, best = max(\n","                        ((compute_iou((tx1,ty1,tx2,ty2), m[\"bbox\"]), m) for m in det_meta),\n","                        key=lambda x: x[0]\n","                    )\n","                    if best_iou > 0.3:\n","                        pts = np.array(best[\"mask_poly\"], np.int32)\n","                        if pts.size:\n","                            cv2.polylines(frame, [pts], isClosed=True, color=MASK_COLOR, thickness=MASK_THICKNESS)\n","                            label_pos = tuple(pts[0])\n","                            cv2.putText(frame, best[\"object_name\"], label_pos, TEXT_FONT,\n","                                        TEXT_SCALE, TEXT_COLOR, TEXT_THICKNESS, cv2.LINE_AA)\n","\n","                        annotations.append({\n","                            \"video_filename\":    video_file,\n","                            \"frame_id\":          frame_id,\n","                            \"track_id\":          f\"{video_name}_{track_id}\",\n","                            \"object_id\":         f\"{frame_id}_obj{track_id}\",\n","                            \"timestamp_sec\":     timestamp_sec,\n","                            \"image_width_px\":    width,\n","                            \"image_height_px\":   height,\n","                            \"MID\":               best[\"MID\"],\n","                            \"object_name\":       best[\"object_name\"],\n","                            \"object_category\":   best[\"object_category\"],\n","                            \"x_min\":             int(tx1),\n","                            \"y_min\":             int(ty1),\n","                            \"x_max\":             int(tx2),\n","                            \"y_max\":             int(ty2),\n","                            \"segmentation_mask\": json.dumps([best[\"mask_poly\"]]),\n","                            \"confidence\":        best[\"confidence\"],\n","                            \"interaction_score\": 0.0\n","                        })\n","\n","        # write frame (with masks) to output\n","        writer.write(frame)\n","        frame_num += 1\n","\n","    cap.release()\n","    writer.release()\n","\n","# write CSV of annotations\n","out_csv = OUTPUT_FOLDER / \"draft_annotations.csv\"\n","pd.DataFrame(annotations).to_csv(out_csv, index=False)\n","print(f\"Saved annotated video(s) to {OUTPUT_FOLDER}\")\n","print(f\"Saved annotations to {out_csv}\")"],"metadata":{"id":"u-d5yUay-sv_","colab":{"base_uri":"https://localhost:8080/","height":432},"executionInfo":{"status":"error","timestamp":1751400718411,"user_tz":420,"elapsed":5712,"user":{"displayName":"Greg Burns","userId":"16420449671066053506"}},"outputId":"0aa943dd-8a1b-49cc-8690-b549498d1598"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement detectron2 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for detectron2\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'detectron2'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-16-2814714125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDefaultPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["## **C) TensorFlow Object Detection API + DeepLab**"],"metadata":{"id":"uGl--7_N-ugB"}},{"cell_type":"code","source":["# Combine TensorFlow Object Detection API + DeepLab + DeepSORT + Mask Drawing\n","# === Install required packages (run once) ===\n","!pip install tensorflow tensorflow-hub tensorflow-object-detection-api deep-sort-realtime opencv-python pandas\n","\n","from google.colab import drive\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from object_detection.utils import label_map_util\n","from deep_sort_realtime.deepsort_tracker import DeepSort\n","import cv2\n","import os\n","import json\n","import pandas as pd\n","from pathlib import Path\n","import numpy as np\n","\n","# === Parameters ===\n","VIDEO_FOLDER         = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Input\")\n","OUTPUT_FOLDER        = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Output\")\n","OD_MODEL_PATH        = Path(\"/content/drive/MyDrive/FreeFuse_Project/models/ssd_mobilenet_v2_coco/saved_model\")\n","LABEL_MAP_PATH       = Path(\"/content/drive/MyDrive/FreeFuse_Project/models/mscoco_label_map.pbtxt\")\n","DEEPLAB_MODEL_URL    = \"https://tfhub.dev/tensorflow/deeplabv3/1\"\n","\n","CONFIDENCE_THRESHOLD = 0.5\n","DETECTION_INTERVAL   = 5      # analyze every Nth frame\n","MAX_TRACK_AGE        = 30     # frames to keep a lost track\n","MIN_HITS             = 3      # detections before confirming a track\n","\n","# drawing settings\n","MASK_COLOR           = (0, 255, 0)    # BGR mask outline color\n","MASK_THICKNESS       = 2              # mask polygon line thickness\n","TEXT_COLOR           = (255, 255, 255)# BGR text color\n","TEXT_FONT            = cv2.FONT_HERSHEY_SIMPLEX\n","TEXT_SCALE           = 0.6\n","TEXT_THICKNESS       = 2\n","\n","# === 1) Mount Google Drive ===\n","drive.mount('/content/drive')\n","\n","# === 2) Load TensorFlow OD model ===\n","detect_fn = tf.saved_model.load(str(OD_MODEL_PATH))\n","category_index = label_map_util.create_category_index_from_labelmap(\n","    str(LABEL_MAP_PATH), use_display_name=True)\n","\n","# === 3) Load DeepLab semantic segmentation model ===\n","seg_model = hub.load(DEEPLAB_MODEL_URL)\n","\n","# Initialize DeepSORT tracker\n","tracker = DeepSort(max_age=MAX_TRACK_AGE, n_init=MIN_HITS)\n","\n","# IoU utility\n","def compute_iou(boxA, boxB):\n","    xA1,yA1,xA2,yA2 = boxA\n","    xB1,yB1,xB2,yB2 = boxB\n","    xi1, yi1 = max(xA1,xB1), max(yA1,yB1)\n","    xi2, yi2 = min(xA2,xB2), min(yA2,yB2)\n","    inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n","    union = (xA2-xA1)*(yA2-yA1) + (xB2-xB1)*(yB2-yB1) - inter\n","    return inter/union if union>0 else 0\n","\n","annotations = []\n","\n","# ensure output paths exist\n","OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n","\n","for video_file in os.listdir(VIDEO_FOLDER):\n","    if not video_file.lower().endswith(('.mp4','.mov','.avi')):\n","        continue\n","\n","    cap        = cv2.VideoCapture(str(VIDEO_FOLDER/video_file))\n","    fps        = cap.get(cv2.CAP_PROP_FPS)\n","    width      = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    frame_num  = 0\n","    video_name = Path(video_file).stem\n","\n","    # prepare video writer\n","    output_path = OUTPUT_FOLDER / video_file\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # run detection and segmentation at intervals\n","        if frame_num % DETECTION_INTERVAL == 0:\n","            timestamp_sec = int(frame_num / fps)\n","            frame_id      = f\"{video_name}_{timestamp_sec:04d}\"\n","\n","            # prepare input for TF models\n","            input_tensor = tf.convert_to_tensor(frame)\n","            input_tensor = input_tensor[tf.newaxis, ...]\n","\n","            # Object Detection inference\n","            detections = detect_fn(input_tensor)\n","            boxes_norm  = detections['detection_boxes'][0].numpy()\n","            classes     = detections['detection_classes'][0].numpy().astype(np.int32)\n","            scores      = detections['detection_scores'][0].numpy()\n","\n","            # Semantic segmentation inference\n","            seg_input = tf.image.convert_image_dtype(frame, tf.uint8)[tf.newaxis, ...]\n","            seg_output = seg_model(seg_input)['default']  # shape: [1, H, W, num_classes]\n","            seg_map = tf.argmax(seg_output, axis=-1)[0].numpy().astype(np.uint8)\n","\n","            dets_for_tracker = []\n","            det_meta = []\n","            for idx in range(len(scores)):\n","                conf = float(scores[idx])\n","                if conf < CONFIDENCE_THRESHOLD:\n","                    continue\n","\n","                # convert normalized box coords to pixels\n","                y1,x1,y2,x2 = boxes_norm[idx]\n","                x1, y1 = int(x1 * width), int(y1 * height)\n","                x2, y2 = int(x2 * width), int(y2 * height)\n","                name = category_index[classes[idx]]['name']\n","\n","                # crop semantic mask to detection box and binarize\n","                crop_mask = (seg_map[y1:y2, x1:x2] > 0).astype(np.uint8) * 255\n","                contours, _ = cv2.findContours(crop_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","                # shift contour points by (x1,y1)\n","                poly = []\n","                if contours:\n","                    cnt = max(contours, key=cv2.contourArea)\n","                    cnt = cnt.reshape(-1, 2) + np.array([x1, y1])\n","                    poly = cnt.tolist()\n","\n","                dets_for_tracker.append([[x1, y1, x2-x1, y2-y1], conf, name])\n","                det_meta.append({\n","                    'bbox': (x1, y1, x2, y2),\n","                    'MID':   f\"/m/{classes[idx]:07d}\",\n","                    'object_name': name,\n","                    'object_category': 'unknown',\n","                    'mask_poly': poly,\n","                    'confidence': conf,\n","                })\n","\n","            # update tracker\n","            tracks = tracker.update_tracks(dets_for_tracker, frame=frame)\n","            if det_meta:\n","                for trk in tracks:\n","                    if not trk.is_confirmed():\n","                        continue\n","                    tx1, ty1, tx2, ty2 = trk.to_tlbr()\n","                    track_id = trk.track_id\n","\n","                    # match detection by IoU\n","                    best_iou, best = max(\n","                        ((compute_iou((tx1, ty1, tx2, ty2), m['bbox']), m) for m in det_meta),\n","                        key=lambda x: x[0]\n","                    )\n","                    if best_iou > 0.3 and best['mask_poly']:\n","                        pts = np.array(best['mask_poly'], np.int32)\n","                        cv2.polylines(frame, [pts], isClosed=True, color=MASK_COLOR, thickness=MASK_THICKNESS)\n","                        cv2.putText(frame, best['object_name'], tuple(pts[0]), TEXT_FONT,\n","                                    TEXT_SCALE, TEXT_COLOR, TEXT_THICKNESS, cv2.LINE_AA)\n","\n","                        annotations.append({\n","                            'video_filename':  video_file,\n","                            'frame_id':        frame_id,\n","                            'track_id':        f\"{video_name}_{track_id}\",\n","                            'object_id':       f\"{frame_id}_obj{track_id}\",\n","                            'timestamp_sec':   timestamp_sec,\n","                            'image_width_px':  width,\n","                            'image_height_px': height,\n","                            'MID':             best['MID'],\n","                            'object_name':     best['object_name'],\n","                            'object_category': best['object_category'],\n","                            'x_min':           int(tx1),\n","                            'y_min':           int(ty1),\n","                            'x_max':           int(tx2),\n","                            'y_max':           int(ty2),\n","                            'segmentation_mask': json.dumps([best['mask_poly']]),\n","                            'confidence':      best['confidence'],\n","                            'interaction_score': 0.0\n","                        })\n","\n","        # write processed frame to output video\n","        writer.write(frame)\n","        frame_num += 1\n","\n","    cap.release()\n","    writer.release()\n","\n","# write CSV annotations\n","out_csv = OUTPUT_FOLDER / 'draft_annotations.csv'\n","pd.DataFrame(annotations).to_csv(out_csv, index=False)\n","print(f\"Saved annotated video(s) to {OUTPUT_FOLDER}\")\n","print(f\"Saved annotations to {out_csv}\")\n"],"metadata":{"id":"kNP76ipE-v1t","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1751400780666,"user_tz":420,"elapsed":40185,"user":{"displayName":"Greg Burns","userId":"16420449671066053506"}},"outputId":"1db76c91-5873-4413-8878-746e8e8019d1"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.11/dist-packages (0.16.1)\n","Collecting tensorflow-object-detection-api\n","  Downloading tensorflow_object_detection_api-0.1.1.tar.gz (577 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.4/577.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: deep-sort-realtime in /usr/local/lib/python3.11/dist-packages (1.3.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub) (2.18.0)\n","Requirement already satisfied: Pillow>=1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-object-detection-api) (11.2.1)\n","Requirement already satisfied: Matplotlib>=2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-object-detection-api) (3.10.0)\n","Requirement already satisfied: Cython>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-object-detection-api) (3.0.12)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from tensorflow-object-detection-api) (5.4.0)\n","Collecting jupyter (from tensorflow-object-detection-api)\n","  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n","Collecting contextlib2 (from tensorflow-object-detection-api)\n","  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow-object-detection-api) (0.45.1)\n","Collecting twine (from tensorflow-object-detection-api)\n","  Downloading twine-6.1.0-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from deep-sort-realtime) (1.15.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (4.58.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (3.2.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->tensorflow-object-detection-api) (6.5.7)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->tensorflow-object-detection-api) (6.1.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->tensorflow-object-detection-api) (7.16.6)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter->tensorflow-object-detection-api) (6.17.1)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter->tensorflow-object-detection-api) (7.7.1)\n","Collecting jupyterlab (from jupyter->tensorflow-object-detection-api)\n","  Downloading jupyterlab-4.4.4-py3-none-any.whl.metadata (16 kB)\n","Collecting readme-renderer>=35.0 (from twine->tensorflow-object-detection-api)\n","  Downloading readme_renderer-44.0-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from twine->tensorflow-object-detection-api) (1.0.0)\n","Requirement already satisfied: keyring>=15.1 in /usr/local/lib/python3.11/dist-packages (from twine->tensorflow-object-detection-api) (25.6.0)\n","Collecting rfc3986>=1.4.0 (from twine->tensorflow-object-detection-api)\n","  Downloading rfc3986-2.0.0-py2.py3-none-any.whl.metadata (6.6 kB)\n","Collecting id (from twine->tensorflow-object-detection-api)\n","  Downloading id-1.5.0-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (3.3.3)\n","Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (0.9.0)\n","Requirement already satisfied: importlib_metadata>=4.11.4 in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (8.7.0)\n","Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (3.4.0)\n","Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (4.2.1)\n","Requirement already satisfied: jaraco.context in /usr/local/lib/python3.11/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (6.0.1)\n","Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine->tensorflow-object-detection-api)\n","  Downloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n","Requirement already satisfied: docutils>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from readme-renderer>=35.0->twine->tensorflow-object-detection-api) (0.21.2)\n","Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from readme-renderer>=35.0->twine->tensorflow-object-detection-api) (2.19.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (1.8.0)\n","Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (7.34.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (6.1.12)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (0.1.7)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (1.6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (5.9.5)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (24.0.1)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (6.4.2)\n","Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (5.7.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (3.6.10)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (3.0.15)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (3.0.51)\n","Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->tensorflow-object-detection-api) (0.28.1)\n","Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->tensorflow-object-detection-api) (3.1.6)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->tensorflow-object-detection-api) (5.8.1)\n","Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n","Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->tensorflow-object-detection-api) (0.2.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (4.13.4)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->tensorflow-object-detection-api) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (3.1.3)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.10.2)\n","Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (5.10.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (1.5.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (25.1.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.22.1)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (1.3.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->tensorflow-object-detection-api) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->tensorflow-object-detection-api) (1.4.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->tensorflow-object-detection-api) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->tensorflow-object-detection-api) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->tensorflow-object-detection-api) (0.16.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.11.4->keyring>=15.1->twine->tensorflow-object-detection-api) (3.23.0)\n","Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter->tensorflow-object-detection-api)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->tensorflow-object-detection-api) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->tensorflow-object-detection-api) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->tensorflow-object-detection-api) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->tensorflow-object-detection-api) (4.9.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter->tensorflow-object-detection-api) (4.3.8)\n","Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter->tensorflow-object-detection-api)\n","  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n","Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n","Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n","Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api) (1.8.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->tensorflow-object-detection-api) (21.2.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->tensorflow-object-detection-api) (0.4)\n","Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->tensorflow-object-detection-api) (2.17.0)\n","Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n","Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->tensorflow-object-detection-api) (4.24.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->tensorflow-object-detection-api) (2.21.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->tensorflow-object-detection-api) (0.2.13)\n","Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.11/dist-packages (from SecretStorage>=3.2->keyring>=15.1->twine->tensorflow-object-detection-api) (43.0.3)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter->tensorflow-object-detection-api) (0.7.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->tensorflow-object-detection-api) (2.7)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from jaraco.classes->keyring>=15.1->twine->tensorflow-object-detection-api) (10.7.0)\n","Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.11/dist-packages (from jaraco.context->keyring>=15.1->twine->tensorflow-object-detection-api) (1.2.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->tensorflow-object-detection-api) (1.3.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine->tensorflow-object-detection-api) (1.17.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->tensorflow-object-detection-api) (0.8.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->tensorflow-object-detection-api) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->tensorflow-object-detection-api) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->tensorflow-object-detection-api) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->tensorflow-object-detection-api) (0.25.1)\n","Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api) (6.0.2)\n","Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine->tensorflow-object-detection-api) (2.22)\n","Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api) (3.0.0)\n","Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api) (24.11.1)\n","Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n","Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->tensorflow-object-detection-api)\n","  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n","Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n","Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n","Downloading twine-6.1.0-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading readme_renderer-44.0-py3-none-any.whl (13 kB)\n","Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n","Downloading id-1.5.0-py3-none-any.whl (13 kB)\n","Downloading jupyterlab-4.4.4-py3-none-any.whl (12.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n","Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nh3-0.2.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.0/739.0 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n","Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n","Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n","Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n","Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n","Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n","Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: tensorflow-object-detection-api\n","  Building wheel for tensorflow-object-detection-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorflow-object-detection-api: filename=tensorflow_object_detection_api-0.1.1-py3-none-any.whl size=844488 sha256=1bcc0cb988e6ed07619aa2d8bee0214b42c2475e7cd2d61d37b92eb25ce359ea\n","  Stored in directory: /root/.cache/pip/wheels/cf/4f/d0/e711451dedf65c9f1d357a34438b4d26fb24a819a22d58a9a6\n","Successfully built tensorflow-object-detection-api\n","Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3986, rfc3339-validator, python-json-logger, overrides, nh3, json5, jedi, fqdn, contextlib2, async-lru, readme-renderer, jupyter-server-terminals, jupyter-client, id, arrow, isoduration, twine, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, tensorflow-object-detection-api\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter-client 6.1.12\n","    Uninstalling jupyter-client-6.1.12:\n","      Successfully uninstalled jupyter-client-6.1.12\n","  Attempting uninstall: jupyter-server\n","    Found existing installation: jupyter-server 1.16.0\n","    Uninstalling jupyter-server-1.16.0:\n","      Successfully uninstalled jupyter-server-1.16.0\n","Successfully installed arrow-1.3.0 async-lru-2.0.5 contextlib2-21.6.0 fqdn-1.5.1 id-1.5.0 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.4 jupyterlab-server-2.27.3 nh3-0.2.21 overrides-7.7.0 python-json-logger-3.3.0 readme-renderer-44.0 rfc3339-validator-0.1.4 rfc3986-2.0.0 rfc3986-validator-0.1.1 tensorflow-object-detection-api-0.1.1 twine-6.1.0 types-python-dateutil-2.9.0.20250516 uri-template-1.3.0\n"]},{"output_type":"error","ename":"TypeError","evalue":"Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-18-1594415116.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeep_sort_realtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepsort_tracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepSort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/object_detection/utils/label_map_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_int_label_map_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/object_detection/protos/string_int_label_map_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mcontaining_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   fields=[\n\u001b[0;32m---> 36\u001b[0;31m     _descriptor.FieldDescriptor(\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object_detection.protos.StringIntLabelMapItem.name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpp_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[0;32m--> 621\u001b[0;31m       \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"]}]},{"cell_type":"markdown","source":["## **D) MMDetection / MMTracking**"],"metadata":{"id":"DRv66FBR-scE"}},{"cell_type":"code","source":["# Combine MMDetection + MMTracking + Mask Drawing\n","# === Install required packages (run once) ===\n","!pip install mmcv-full mmdet mmtrack opencv-python pandas\n","\n","from google.colab import drive\n","import cv2\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from mmdet.apis import init_detector, inference_detector\n","from mmtrack.apis import init_model as init_mot, inference_mot\n","\n","# === Parameters ===\n","VIDEO_FOLDER         = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Input\")\n","OUTPUT_FOLDER        = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Output\")\n","DET_CFG              = \"/content/drive/MyDrive/FreeFuse_Project/configs/mmdet/mask_rcnn_r50_fpn_3x_coco.py\"\n","DET_CKPT             = \"/content/drive/MyDrive/FreeFuse_Project/checkpoints/mask_rcnn_r50_fpn_3x_coco.pth\"\n","MOT_CFG              = \"/content/drive/MyDrive/FreeFuse_Project/configs/mmtrack/bytertrack_faster-rcnn_fpn_4e_mot17-private-half.py\"\n","MOT_CKPT             = \"/content/drive/MyDrive/FreeFuse_Project/checkpoints/bytetrack_faster-rcnn_fpn_mot17.pth\"\n","CONFIDENCE_THRESHOLD = 0.5\n","DETECTION_INTERVAL   = 5      # analyze every Nth frame\n","\n","# drawing settings\n","MASK_COLOR           = (0, 255, 0)    # BGR mask outline\n","MASK_THICKNESS       = 2              # mask polygon line thickness\n","TEXT_COLOR           = (255, 255, 255)# label color\n","TEXT_FONT            = cv2.FONT_HERSHEY_SIMPLEX\n","TEXT_SCALE           = 0.6\n","TEXT_THICKNESS       = 2\n","\n","# === 1) Mount Google Drive ===\n","drive.mount('/content/drive')\n","\n","# === 2) Initialize MMDetection & MMTracking ===\n","det_model = init_detector(DET_CFG, DET_CKPT, device='cuda:0')\n","mot_model = init_mot(MOT_CFG, MOT_CKPT, device='cuda:0')\n","\n","# utility to compute IoU\n","def compute_iou(boxA, boxB):\n","    xA1,yA1,xA2,yA2 = boxA\n","    xB1,yB1,xB2,yB2 = boxB\n","    xi1, yi1 = max(xA1,xB1), max(yA1,yB1)\n","    xi2, yi2 = min(xA2,xB2), min(yA2,yB2)\n","    inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n","    union = (xA2-xA1)*(yA2-yA1) + (xB2-xB1)*(yB2-yB1) - inter\n","    return inter/union if union>0 else 0\n","\n","annotations = []\n","OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n","\n","for video_file in os.listdir(VIDEO_FOLDER):\n","    if not video_file.lower().endswith(('.mp4','.mov','.avi')):\n","        continue\n","    cap = cv2.VideoCapture(str(VIDEO_FOLDER / video_file))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    frame_num = 0\n","    video_name = Path(video_file).stem\n","\n","    # setup writer\n","    out_path = OUTPUT_FOLDER / video_file\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    writer = cv2.VideoWriter(str(out_path), fourcc, fps, (w, h))\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_num % DETECTION_INTERVAL == 0:\n","            timestamp = int(frame_num / fps)\n","            frame_id = f\"{video_name}_{timestamp:04d}\"\n","\n","            # 1) Detect & segment with MMDetection\n","            det_results = inference_detector(det_model, frame)\n","            bbox_results, mask_results = det_results\n","\n","            # 2) Track with MMTracking\n","            track_results, _ = inference_mot(mot_model, det_results, frame)\n","            # track_results: list of dict with 'track_bboxes', 'track_ids'\n","            tracks = track_results[0]\n","            track_bboxes = tracks['track_bboxes']  # np.ndarray[N,5]\n","            track_ids    = tracks['track_ids']     # list of N ids\n","\n","            # 3) Draw masks per track\n","            for cls_id, bboxes in enumerate(bbox_results):\n","                for i, bbox in enumerate(bboxes):\n","                    score = float(bbox[4])\n","                    if score < CONFIDENCE_THRESHOLD:\n","                        continue\n","                    x1,y1,x2,y2 = map(int, bbox[:4])\n","\n","                    # find corresponding track ID by IoU\n","                    best_iou, best_idx = 0, -1\n","                    for idx, tb in enumerate(track_bboxes):\n","                        iou = compute_iou((x1,y1,x2,y2), tb[:4])\n","                        if iou > best_iou:\n","                            best_iou, best_idx = iou, idx\n","                    if best_iou < 0.3:\n","                        continue\n","                    track_id = track_ids[best_idx]\n","\n","                    # extract mask polygon\n","                    mask = mask_results[cls_id][i]  # binary mask\n","                    mask_u8 = (mask.astype(np.uint8) * 255)\n","                    contours, _ = cv2.findContours(mask_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","                    if not contours:\n","                        continue\n","                    pts = max(contours, key=cv2.contourArea).reshape(-1,2)\n","\n","                    # draw\n","                    cv2.polylines(frame, [pts], True, MASK_COLOR, MASK_THICKNESS)\n","                    cv2.putText(frame, det_model.CLASSES[cls_id], tuple(pts[0]), TEXT_FONT,\n","                                TEXT_SCALE, TEXT_COLOR, TEXT_THICKNESS, cv2.LINE_AA)\n","\n","                    # record\n","                    annotations.append({\n","                        'video_filename': video_file,\n","                        'frame_id': frame_id,\n","                        'track_id': f\"{video_name}_{track_id}\",\n","                        'object_id': f\"{frame_id}_obj{track_id}\",\n","                        'timestamp_sec': timestamp,\n","                        'image_width_px': w,\n","                        'image_height_px': h,\n","                        'MID': f\"/m/{cls_id:07d}\",\n","                        'object_name': det_model.CLASSES[cls_id],\n","                        'object_category': 'unknown',\n","                        'x_min': x1, 'y_min': y1, 'x_max': x2, 'y_max': y2,\n","                        'segmentation_mask': json.dumps([pts.tolist()]),\n","                        'confidence': score,\n","                        'interaction_score': 0.0\n","                    })\n","\n","        writer.write(frame)\n","        frame_num += 1\n","\n","    cap.release()\n","    writer.release()\n","\n","# export CSV\n","out_csv = OUTPUT_FOLDER / 'draft_annotations.csv'\n","pd.DataFrame(annotations).to_csv(out_csv, index=False)\n","print(f\"Saved videos to {OUTPUT_FOLDER}\")\n","print(f\"Saved annotations to {out_csv}\")"],"metadata":{"id":"vaQaFuq1-wro","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1751403388903,"user_tz":420,"elapsed":2597712,"user":{"displayName":"Greg Burns","userId":"16420449671066053506"}},"outputId":"e93ffc47-91e0-4ff1-b71b-e2ba179113da"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mmcv-full\n","  Downloading mmcv-full-1.7.2.tar.gz (607 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting mmdet\n","  Downloading mmdet-3.3.0-py3-none-any.whl.metadata (29 kB)\n","Collecting mmtrack\n","  Downloading mmtrack-0.14.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Collecting addict (from mmcv-full)\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (24.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (11.2.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (6.0.2)\n","Collecting yapf (from mmcv-full)\n","  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mmdet) (3.10.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from mmdet) (2.0.10)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mmdet) (1.15.3)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from mmdet) (2.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from mmdet) (1.17.0)\n","Collecting terminaltables (from mmdet)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mmdet) (4.67.1)\n","Collecting attributee (from mmtrack)\n","  Downloading attributee-0.1.9.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting dotty-dict (from mmtrack)\n","  Downloading dotty_dict-1.3.1-py3-none-any.whl.metadata (5.3 kB)\n","Collecting lap (from mmtrack)\n","  Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n","Collecting mmcls<1.0.0,>=0.16.0 (from mmtrack)\n","  Downloading mmcls-0.25.0-py2.py3-none-any.whl.metadata (14 kB)\n","Collecting motmetrics (from mmtrack)\n","  Downloading motmetrics-1.4.0-py3-none-any.whl.metadata (20 kB)\n","Collecting pandas\n","  Downloading pandas-1.3.5.tar.gz (4.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting scipy (from mmdet)\n","  Downloading scipy-1.6.1.tar.gz (27.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from mmtrack) (0.13.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (4.58.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (3.2.3)\n","Collecting xmltodict>=0.12.0 (from motmetrics->mmtrack)\n","  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->mmcv-full) (4.3.8)\n","Downloading mmdet-3.3.0-py3-none-any.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmtrack-0.14.0-py3-none-any.whl (400 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.4/400.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmcls-0.25.0-py2.py3-none-any.whl (648 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.8/648.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Downloading dotty_dict-1.3.1-py3-none-any.whl (7.0 kB)\n","Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n","Building wheels for collected packages: mmcv-full, pandas, scipy, attributee\n","  Building wheel for mmcv-full (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mmcv-full: filename=mmcv_full-1.7.2-cp311-cp311-linux_x86_64.whl size=27596991 sha256=c17ebda66e0479bc8abc05e1a0023d52e1021b1e6da2d0a27fa0ba87844adfe8\n","  Stored in directory: /root/.cache/pip/wheels/4a/27/7f/15d0b51889e45303437b10644ee233f66d2e4f164e3a7c2b20\n","  Building wheel for pandas (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pandas: filename=pandas-1.3.5-cp311-cp311-linux_x86_64.whl size=37464041 sha256=d7e6d9dd08dfc11ec4b893c7da4be9108850bdd65a685a8052398fd20a60e4ad\n","  Stored in directory: /root/.cache/pip/wheels/8b/e7/6d/d4c288f419ab8fa07c1db6f606a2ae18ecf3dc2839d79a1c07\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for scipy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for scipy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for scipy\u001b[0m\u001b[31m\n","\u001b[0m  Building wheel for attributee (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for attributee: filename=attributee-0.1.9-py3-none-any.whl size=13102 sha256=2e455aca33b55a9e015bcc502096e3265e495db1d8ec2e4263df5379a5234542\n","  Stored in directory: /root/.cache/pip/wheels/02/cd/56/376a2b69568637af23ad07e8c356d32767d2f4e78f69b83b06\n","Successfully built mmcv-full pandas attributee\n","Failed to build scipy\n","\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scipy)\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'mmdet'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-19-300984969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_detector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmtrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minit_mot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_mot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmdet'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}