# Hackathon & Competition Projects

This folder contains projects completed as part of competitive data science events. These notebooks showcase end-to-end machine learning workflows under tight deadlines, from exploratory data analysis and feature engineering to model tuning and submission.

---

### 1. Restaurant Annual Turnover Prediction (2nd Place Finish)

* [cite_start]**Event:** Great Learning Hackathon (June 2025) [cite: 1]
* [cite_start]**Objective:** To build a machine learning model to accurately predict the annual turnover of restaurants across India based on a diverse set of features. [cite: 1]
* [cite_start]**Approach:** The project involved comprehensive exploratory data analysis (EDA), advanced feature engineering (including log transformation of the target variable and creating interaction terms), and robust model validation using 5-Fold Cross-Validation. [cite: 1]
* [cite_start]**Technologies Used:** Python, Pandas, Scikit-learn, CatBoost, XGBoost, and Optuna for Bayesian hyperparameter tuning. [cite: 1]
* [cite_start]**Outcome:** **Secured 2nd Place out of 107 participating teams.** The final, highest-ranking submission was a fine-tuned CatBoostRegressor model. [cite: 1]

---

### 2. Kaggle's Spaceship Titanic: Survival Prediction

* [cite_start]**Event:** Kaggle Competition [cite: 2]
* [cite_start]**Objective:** A binary classification task to predict which passengers were "transported to an alternate dimension" following a spacetime anomaly, based on passenger and spaceship data. [cite: 2]
* [cite_start]**Approach:** The project involved detailed EDA, data imputation to handle missing values, feature engineering (such as creating a 'GroupSize' feature), and the training and comparison of a full suite of classification models. [cite: 2]
* [cite_start]**Technologies Used:** Python, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn (Logistic Regression, KNN, SVM, Decision Tree, Random Forest), XGBoost, CatBoost, and Ensemble Methods (Voting Classifier). [cite: 2]
* [cite_start]**Outcome:** A highly accurate ensemble model was developed for the final submission, combining the predictive power of several fine-tuned classifiers to maximize the competition score. [cite: 2]
